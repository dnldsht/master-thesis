\chapter{Experiments}

In this chapter, we delve into the experiments conducted to optimize the performance of our models.
To achieve the best possible results, we thoroughly explored the hyperparameter space, testing various parameters such as learning rate, batch size, activation functions, and regularization techniques.
Additionally, we experimented with modifications to the architecture of the networks based on the results reported by the original authors.
This process allowed us to identify the optimal settings for each model on our specific dataset. 
Although we encountered challenges such as overfitting and convergence problems, we overcame them through careful experimentation and iteration.

Furthermore, we address the impact of missing data on the performance of our models in time series classification.
We implemented two different approaches to handle missing values: one involved replacing the missing values with zeros, while the other used pre-imputed values based on linear multi-temporal interpolation \cite{IENCO201911}.
By comparing the performance of the models with both methods, we gained a better understanding of how missing data affects model accuracy.


% \section{Old Method}
% The main objective of this thesis is to perform a comprehensive comparison of different methods for time series classification, especially those dealing with missing data.
% To achieve this goal, we have implemented several machine learning models and evaluated their effectiveness on a comprehensive dataset of satellite image time series.
% The comparison and evaluation of these models will help to determine the most effective approach to handle missing data and improve the accuracy of time series classification tasks.


\section{Data preparation}

To ensure a comprehensive and unbiased comparison, the same dataset and evaluation metrics were used for all machine learning models in this thesis. 
Specifically, the dataset of choice was the Satellite Image Time Series (SITS) dataset, which contains a total of 137,606 time series of satellite images covering natural and semi-natural areas. 
Since the data available for this task is relatively limited, k-fold cross validation with $k=5$ and varying seeds was implemented to ensure that the results were not affected by the partitioning of the dataset.
Additionally, to ensure a balanced distribution of classes within each fold, the dataset was partitioned into training/validation/test sets with equal representation of each class. 
In order to prevent spatial correlation, we avoided placing identical polygon pixels within the same sets. 
This precaution was taken because disregarding spatio-temporal autocorrelation \cite{doi:10.1080/10095020.2019.1643609} may result in biased and unreliable outcomes.

The distribution of polygons and pixels for each class in the training, validation, and test sets can be observed in Table \ref{tab:dataset-splits}.
The splitting was done in a ratio of 60:20:20 for the training, validation, and test sets respectively.
The table provides a comprehensive summary of the number of polygons and pixels for each class in each of the sets.

\begin{table}[H]
  \begin{tabular}{crrlcrrlcrrl}
     \cline{2-4} \cline{6-8} \cline{10-12} \\[-0.2cm]
          & \multicolumn{3}{c}{Train}                                 & \multicolumn{1}{l}{} & \multicolumn{3}{c}{Validation}                            & \multicolumn{1}{c}{} & \multicolumn{3}{c}{Test} \\[0.1cm]
    Class & Pol.                    & Pix.                      & \%  &                      & Pol.                    & Pix.  & \%                      &                      & Pol.   & Pix.    & \%    \\[0.2cm] \cline{2-4} \cline{6-8} \cline{10-12} \\[-0.2cm]
    1     & 427                     & 6486                      & 7   &                      & 142                     & 2266  & 8                       &                      & 143    & 1660    & 6     \\
    2     & 196                     & 3369                      & 3   &                      & 65                      & 803   & 3                       &                      & 67     & 993     & 3     \\
    3     & 49                      & 21298                     & 24  &                      & 16                      & 5308  & 20                      &                      & 17     & 5489    & 21    \\
    4     & 103                     & 30631                     & 35  &                      & 34                      & 8194  & 31                      &                      & 35     & 10350   & 40    \\
    5     & 133                     & 16933                     & 19  &                      & 44                      & 6362  & 24                      &                      & 46     & 5372    & 20    \\
    6     & 19                      & 1428                      & 1   &                      & 6                       & 309   & 1                       &                      & 7      & 338     & 1     \\
    7     & 23                      & 3399                      & 3   &                      & 7                       & 948   & 3                       &                      & 9      & 858     & 3     \\
    8     & 33                      & 2595                      & 3   &                      & 11                      & 1455  & 5                       &                      & 12     & 762     & 2     \\[0.2cm]\hline \\[-0.2cm]
    Tot   & \multicolumn{1}{l}{983} & \multicolumn{1}{l}{86139} & 100 & \multicolumn{1}{l}{} & \multicolumn{1}{l}{325} & 25645 & \multicolumn{1}{l}{100} & \multicolumn{1}{l}{} & 336    & 25822   & 100       
  \end{tabular}
  \caption{Distribution of classes, polygons and pixels for each dataset.}
  \label{tab:dataset-splits}
\end{table}

\input{contents/experiments/rf}
\pagebreak
\input{contents/experiments/tempcnn}
\pagebreak
\input{contents/experiments/ajrnn}
\pagebreak
\input{contents/experiments/ltae}
\pagebreak

\section{Comparing results}

The performance of the different models on both the imputed and non-imputed datasets is compared in this section.

\begin{table}[H]
  \centering
    \begin{tabular}{lr}
    Model                       & Overall Accuracy             \\[0.2cm] 
    \hline \\[-0.2cm]
    RF            & $91.03 \pm 0.42$\\
    Light AJ-RNN  & $87.29 \pm 1.08$\\
    TempCNN       & $93.74 \pm 0.03$\\
    L-TAE         & $93.58 \pm 1.29$
    \end{tabular}
  \caption{Overall accuracy of the best models with pre imputed missing values}
  \label{tab:ALLresultsImputed} 
\end{table}

\begin{table}[H]
  \centering
    \begin{tabular}{lr}
    Model                       & Overall Accuracy             \\[0.2cm] 
    \hline \\[-0.2cm]
    RF      & $89.72 \pm 0.84$\\
    AJ-RNN  & $86.07 \pm 1.58$\\
    TempCNN & $92.57 \pm 0.84$\\
    L-TAE   & $93.65 \pm 1.30$
    \end{tabular}
  \caption{Overall accuracy of the best models without imputation of missing values} 
  \label{tab:ALLresultsNoImputed}
\end{table}

Overall, our experiments show that TempCNN and L-TAE outperformed the other models on both datasets.
Specifically, L-TAE achieved an accuracy of 93.58\% and 93.65\% on the imputed and non-imputed datasets, respectively, 
while TempCNN achieved an accuracy of 93.74\% and 92.57\%. RF and AJ-RNN both achieved lower accuracies. 
These results suggest that deep learning models such as TempCNN and L-TAE may be more suitable for modeling satellite image time series data.

\begin{figure}[H]
  \centering
  \includegraphics[height=5.14cm,width=0.47\textwidth]{cfTempCNN}
  \includegraphics[width=0.52\textwidth]{cfLTAE}
  \caption{Confusion matrices for the TempCNN model (left) and L-TAE model (right) on the test dataset. The y-axis represents the true labels, and the x-axis represents the predicted labels.}
\end{figure}

Based on the confusion matrices, both TempCNN and L-TAE models seem to have difficulty distinguishing between the orchard and crop classes, as indicated by the high number of misclassifications between these two classes.
This could be due to the fact that these two classes may share similar temporal patterns in the time series data, making it more difficult for the models to distinguish between them. 
It may be worth exploring additional features or data sources to improve the performance of the models in distinguishing between these two classes.

\subsection{Imputation of missing values}

The inclusion of missing data is often a challenge in machine learning tasks, as missing values can have a significant impact on the performance of the model.
In the first scenario, we addressed this issue by imputing the missing values prior to training the models. 
This allowed us to utilize all available data in the training process, potentially leading to improved performance.

In the second scenario, we did not impute the missing values, which meant that the training process was performed using only the available non-missing data.
This scenario may be particularly useful in cases where imputation may introduce bias or distort the underlying patterns in the data.

Overall, the results reported in Tables \ref{tab:ALLresultsImputed} and \ref{tab:ALLresultsNoImputed} show that the inclusion of missing data through imputation did not significantly affect the performance of the models.