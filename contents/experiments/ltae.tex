\section{L-TAE}
In order to embed the input images into the L-TAE model, we needed to replace the Pixel Set Encoder (PSE) used in the original model with an encoder that was more appropriate for our dataset.

This led to the adoption of the Dense Encoder (DE), which is a linear neural network that takes a 16-channel image as input and uses a hyperbolic tangent (tanh) activation function to generate a 64-dimensional vector.
Our goal in designing the DE was to ensure computational efficiency, allowing faster training and inference times, while maintaining strong performance on our dataset.

\subsection{Experimental results}

After adapting the L-TAE model to our dataset, we trained it by adjusting the hyperparameters to find the optimal configuration, guided by the results of their research.

\begin{table}[ht]
  \centering
  \begin{tabular}{l p{10cm}}   
     Param & Description \\[0.2cm] 
     \hline \\[-0.2cm]  
     E & size of the embeddings ($E$), if input vectors are of a different size, a linear layer is used to project them d\_model-dimensional space \\
     H & Number of attention heads  \\
     K & Dimension of the key and query vectors  \\
     MLP & Number of neurons in the layers of MLP \\
  \end{tabular}
  \caption{Hyper-parameters of L-TAE model}
  \label{tab:LTAEconfig}
\end{table}

In the following, we have described the findings of the authors on the impact of the different hyperparameters on the performance of the model.

\begin{paragraph} {Number of heads} 
It appears that the performance is only marginally impacted by the number of heads. 
Their hypothesis is that an increase in the number of heads ($H$) can be advantageous, but a reduction in group size ($E'$) can be detrimental.
\end{paragraph}

\begin{paragraph} {Dimension of keys}
The experiments indicate that smaller key dimensions, as opposed to the typical values used in NLP or for the TAE ($K = 32$), lead to better performance on the problem.
The L-TAE can achieve comparable performance to the TAE with only 2-dimensional keys.
\end{paragraph}

\begin{paragraph} {Dimension of Input}
The expected outcome of having larger input embeddings is an increase in performance, as it corresponds to a more comprehensive representation.
Nevertheless, on the dataset being considered, the benefits of increasing the number of parameters are diminishing.
\end{paragraph}

\begin{paragraph} {Query-as-Parameter}
To evaluate the impact of the design choices, a variation of the network is trained using the same master-query scheme as the TAE.
The larger linear layer that results increases the model's size to a total of 170k parameters.
The observation that the resulting mIoU is only 49.7 suggests that the query-as-parameter scheme is advantageous not only in terms of compactness but also for achieving better performance.
\end{paragraph}

Table \ref{tab:LTAEresults} shows the performance results of the L-TAE architecture with different configurations of the following hyperparameters: number of heads H, dimension of keys K, and number of channels E in the input sequence.
All results were obtained using a 5-fold cross-validation scheme.
The performance metrics were measured for two different scenarios: one where missing values were imputed and one where missing values were not imputed.

\begin{table}[H]
  \centering
  \begin{tabular}{cccclrr} 
     Params & E & H & K & MLP & No imputation & Pre imputation\\[0.2cm] 
     \hline \\[-0.2cm] 
     43k & 	128 & 	8 & 	8 & 	128 & 	$93.37 \pm 1.65$ & 	$93.08 \pm 1.16$\\ 
     68k & 	128 & 	16 & 	8 & 	128-128 & 	$93.43 \pm 1.37$ & 	$93.39 \pm 1.16$\\ 
     123k & 	256 & 	16 & 	8 & 	256-128 & 	$93.15 \pm 1.72$ & 	$93.44 \pm 1.22$\\ 
     299k & 	512 & 	32 & 	8 & 	512-128 & 	$\mathbf{93.65 \pm 1.30}$ & 	$93.40 \pm 1.16$\\ 
     749k & 	1024 & 	32 & 	8 & 	1024-256-128 & 	$92.91 \pm 1.90$ & 	$\mathbf{93.58 \pm 1.29}$\\ 
  \end{tabular}
  \caption{Results of the L-TAE model with different parameters}
  \label{tab:LTAEresults}
\end{table}

Overall, the results reported in Table \ref{tab:LTAEresults} demonstrate the effectiveness of the L-TAE architecture in accurately classifying the input data even with a lower number of parameters. 
The inclusion of missing data through imputation did not significantly impact the performance of the model, suggesting that the L-TAE architecture is robust to the presence of missing values.

The model has been implemented in Python using the PyTorch library \cite{NEURIPS2019_9015}.
The source code for this model is available at \url{https://github.com/dnldsht/lightweight-temporal-attention-pytorch}, which is a forked version of the orginal L-TAE implementation \cite{LTAE}.