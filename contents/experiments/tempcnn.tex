\section{TempCNN}

In this section, we present the results of our experiments investigating the effectiveness of the TempCNN model on time series data from satellite imagery.
We investigated the effect of the network depth and the width of its convolutional layers on the classification accuracy. 
Furthermore, we investigated the effect of the inclusion of pooling layers and the filter size on the classification accuracy. 
Finally, we investigated the effect of batch size on classification accuracy. 

By thoroughly investigating these factors, we gained insight into how to optimize the TempCNN model for this type of data.

\subsection{Data preparation}

To normalize the data, we used min-max normalization, the same used in \cite{tempCNN}.
The traditional min-max normalization performs a subtraction of the minimum, then a division by the range, i.e., the maximum minus the minimum \cite{han2011data}.
This normalization is very sensitive to extreme values, so we used the 2\% (or 98\%) percentile instead of the minimum (or maximum) value. 
For each feature, both percentile values are extracted from all timestamp values.

\subsection{Experimental results}


\begin{paragraph}{Depth}
  In a convolutional neural network (CNN), depth refers to the number of layers in the network.
  A deeper network can learn more complex features by using a hierarchy of layers, each layer building on the features learned by the previous layer.
  However, increasing the depth can also lead to the vanishing gradient problem, where the gradients become too small to effectively update the weights during training.
  Therefore, finding the optimal depth is a critical consideration when designing a CNN.
\end{paragraph}

To investigate the effect of network depth on model performance while keeping complexity constant, we vary the number of layers in the network while reducing the number of units in deeper layers.
We experiment with six architectures consisting of between one and six convolutional layers, each with a varying number of units ranging from 256 to 16.
Additionally, each architecture includes a dense layer with a number of units ranging from 64 to 2048. 
In each experiment, we train the model twice: first using the dataset with pre-imputed missing values, and then using a modified dataset where all missing values are replaced with zeros.
As shown in the Table \ref{tab:temCNNdepth}, the highest accuracy is achieved with two or three convolutional layers.

\begin{table}[H]
  \centering
   \begin{tabular}{rclrr}
   Model&&                  & No imputation         & Pre imputation             \\[0.2cm]
   \hline \\[-0.2cm]
   1CONV256 &+& 1FC64   	 & $91.39 \pm 0.74$ 	 & $91.12 \pm 0.57$\\
   2CONV128 &+& 1FC128  	 & $91.32 \pm 1.11$ 	 & $91.32 \pm 1.28$\\
   3CONV64 &+& 1FC256   	 & $\mathbf{92.57 \pm 0.84}$ 	 & $\mathbf{92.93 \pm 1.50}$\\
   4CONV32 &+& 1FC512   	 & $92.33 \pm 1.32$ 	 & $90.45 \pm 1.98$\\
   5CONV16 &+& 1FC1024  	 & $91.20 \pm 0.94$ 	 & $88.20 \pm 2.95$\\
   6CONV8 &+& 1FC2048   	 & $87.83 \pm 2.89$ 	 & $88.58 \pm 2.90$\\
   \end{tabular}
   \caption{Influence of depth on classification accuracy.}
   \label{tab:temCNNdepth}
 \end{table}

\begin{paragraph}{Width}
The width of a convolutional neural network (CNN) refers to the number of neurons in each layer.
Increasing the width can enhance the network's ability to learn complex features because more neurons are available for training.
However, a network that is too wide may be prone to overfitting, where it memorizes the training data rather than generalizing to new data.
Therefore, finding the optimal width is critical for achieving the best performance on the test data.
\end{paragraph}

To evaluate the impact of width on model performance, we experimented with seven CNN architectures.
Each architecture includes three convolutional layers, one dense layer with 256 neurons, and a Softmax layer, as illustrated in Figure \ref{fig:temCNNArchitecture}.
The architectures differ in the number of parameters, and we vary the width of the convolutional layers from 16 to 1024 neurons.

 \begin{table}[H]
  \centering
   \begin{tabular}{rclrr}
   Model&&                  & No imputation         & Pre imputation             \\[0.2cm]
   \hline \\[-0.2cm]
   3CONV16 &+& 1FC256    	 & $92.44 \pm 0.83$ 	 & $91.62 \pm 0.41$\\
   3CONV32 &+& 1FC256    	 & $92.52 \pm 0.68$ 	 & $92.16 \pm 0.68$\\
   3CONV64 &+& 1FC256    	 & $92.45 \pm 0.89$ 	 & $\mathbf{93.47 \pm 0.63}$\\
   3CONV128 &+& 1FC256   	 & $92.26 \pm 1.84$ 	 & $92.13 \pm 1.35$\\
   3CONV256 &+& 1FC256   	 & $91.66 \pm 1.96$ 	 & $92.04 \pm 1.67$\\
   3CONV512 &+& 1FC256   	 & $91.97 \pm 1.23$ 	 & $92.75 \pm 1.73$\\
   3CONV1024 &+& 1FC256  	 & $\mathbf{92.76 \pm 1.46}$ 	 & $93.39 \pm 1.54$\\
   \end{tabular}
   \caption{Influence of width on classification accuracy.}
   \label{tab:temCNNwidth}
 \end{table}

The results reported in Table \ref{tab:temCNNwidth} show that the architecture is remarkably robust, even when there is considerable variation in the number of neurons.
This is illustrated by the fact that the overall accuracy shows a difference of only 1.7\% between the model with lower parameters and the one with higher parameters when using pre-imputed data, while the difference is only 0.32\% when there is no imputation for missing values.
Although the standard deviation increases with the number of parameters, the architecture consistently achieves high accuracy rates, indicating that it is capable of maintaining good performance even with a larger number of neurons.

We chose to use the model with three convolutional layers of 64 neurons each and a dense layer of 256 neurons for the upcoming experiments because it offers a favorable balance between bias and variance.
This choice was made considering the size of our training dataset, which consists of 80,000 samples.

\begin{paragraph}{Batch size}
  In machine learning, batch size refers to the number of training examples used in a gradient descent iteration.
  Batch size plays a critical role in determining the efficiency and accuracy of the training process.
  A larger batch size can result in faster training because the algorithm can process more examples in each iteration.
  However, using a larger batch size can also result in a less accurate model because it can cause the optimization algorithm to converge to a suboptimal solution.
  On the other hand, using a smaller batch size may result in slower training, but may help the algorithm converge to a better solution. 
  Thus, choosing an appropriate batch size is an important consideration in machine learning.
\end{paragraph}

We conducted experiments to determine the optimal batch size for training the TempCNN model.
Specifically, we tested four different batch sizes: 16, 32, 64, and 128.
Our analysis, as presented in Table \ref{tab:temCNNbatchsize}, shows that batch size has a noticeable impact on training time; larger batch sizes result in faster training times.
However, we observed that the batch size does not have a significant effect on the overall accuracy of the TempCNN model.
In fact, we found that the accuracy values for all four batch sizes are comparable, indicating that selecting an appropriate batch size is not a critical factor in achieving high accuracy for this model.

\begin{table}[H]
  \centering
   \begin{tabular}{rlll}
   Batch size                 & Train time  & No imputation         & Pre imputation             \\[0.2cm]
   \hline \\[-0.2cm]
    16        & 52min  	 & $91.92 \pm 1.50$ 	 & $92.16 \pm 1.75$\\
    32        & 42min  	 & $\mathbf{91.61 \pm 1.19}$ 	 & $\mathbf{93.74 \pm 0.03}$\\
    64        & 26min  	 & $90.88 \pm 1.63$ 	 & $92.30 \pm 0.89$\\
    128       & 20min  	 & $91.54 \pm 1.69$ 	 & $91.52 \pm 1.67$\\
   \end{tabular}
   \caption{Influence of batch size on training time and classification accuracy.}
   \label{tab:temCNNbatchsize}
 \end{table}
 
\begin{paragraph}{Pooling layers}
Pooling is a common technique used in deep learning for dimensionality reduction and feature extraction.
In the context of computer vision, the two most popular types of pooling are local max-pooling \cite{Ren2015Faster} and global average pooling \cite{He2016Deep}.
Local max-pooling takes the maximum value of a small subregion of a feature map, while global average pooling takes the average of all feature map values.
However, for time series data, global average pooling has been shown to be more effective in previous studies \cite{rs10020236,fawaz2018deep}.
In this work, we investigate whether these findings can be generalized to time series classification tasks. 
Specifically, we will compare the performance of the TempCNN model using both local max-pooling and global average pooling to determine which pooling method is most effective for our specific application.
\end{paragraph}

\begin{paragraph}{Filter size}
Filter Size is an important Hyperparameter in Convolutional Neural Networks (CNNs)
The filter size, also known as the kernel size, determines the receptive field of the convolutional layer and influences the network's ability to capture spatial features in the input.
A larger filter size allows the network to capture more complex patterns and details, but at the cost of increased computation and potential overfitting.
Conversely, a smaller filter size reduces computation and may improve the network's ability to generalize to new data, but at the risk of losing important features.
Therefore, the filter size should be chosen based on the complexity of the task and the size of the input data.
\end{paragraph}



Figure \ref{tab:tempCNNPooling} displays the OA values as a function of filter size. 
Each bar represents a different configuration: local max-pooling (MP), local max-pooling and global average pooling (MP + GAP), local average pooling (AP), local and global average pooling (AP + GAP), and global average pooling (GAP). 
The horizontal magenta dashed line corresponds to the OA values obtained without pooling layers in the previous experiment.


\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{tempCNNPooling.png}
  \caption{Overall Accuracy as a Function of Filter Size for Different Pooling Methods: Local max-pooling (MP) in orange, local max-pooling and global average pooling (MP + GAP) in red, local average pooling (AP) in green, local and global average pooling (AP + GAP) in purple, and global average pooling (GAP) in blue.}
  \label{tab:tempCNNPooling}
\end{figure}

Figure \ref{tab:tempCNNPooling} shows that the use of pooling layers performs poorly: the OA results are almost always
below the one obtained without pooling layers (magenta dashed line). 

\subsection{Findings}

Our experiments on the TempCNN model for satellite image time series data have provided some valuable insights. 
First, we found that a network depth of 3 layers is sufficient to achieve high classification accuracy. 
In addition, we discovered that a width of 64 neurons per layer is sufficient. 
Furthermore, we found that a batch size of 32 is sufficient for training the model. 
Finally, we observed that the use of pooling layers did not contribute significantly to the classification accuracy.

The model has been implemented in Python using the Keras library \cite{chollet2015keras} and the TensorFlow backend \cite{tensorflow2015-whitepaper}.
The source code for this model is available at \url{https://github.com/dnldsht/temporalCNN}, which is a forked version of the orginal TempCNN implementation \cite{tempCNN}.