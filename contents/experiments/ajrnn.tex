\section{AJ-RNN}

The AJ-RNN model proposed in the paper was implemented in Python 2.7 and Tensorflow 1.0.
However, given the changes in technology and advances in the field since the publication of the paper, we found it necessary to re-implement the model in a more recent version of TensorFlow.
As such, we spent a considerable amount of time and effort to re-implement the model in Tensorflow 2.0 in a modularized fashion, with the aim of enhancing code readability and facilitating experimentation with different configurations.

In addition to the re-implementation of the model, we also performed a careful validation process to ensure that the accuracy achieved by our model was consistent with the results reported in the original paper, using the same datasets. 
This involved testing the model on a range of datasets and comparing the results with the original paper. 
By doing so, we were able to ensure that our implementation was both accurate and reliable.

Furthermore, we extended the implementation of the model to support multivariate time series data. 
This undertaking was quite significant as it required extensive additional coding and validation efforts. 
However, it was essential to ensure that the AJ-RNN model could be effectively utilized in various applications, including our own.

% \subsection{Data preparation}

% We understood the significance of the mask vector in the AJ-RNN model, which is used to differentiate between revealed and imputed values.
% Therefore, we generated a mask vector for each sample to feed into the model.

\subsection{Experimental results}

We tried different combinations of hyper-parameters to find the best configuration for our dataset.
The hyper-parameters we tuned are the RNN cell type, the units of the cell, the learning rate, the batch size, and the number of epochs for the generator and discriminator.

In our experiments, we investigated the performance of the AJ-RNN model with both LSTM and GRU cells. 
We tested different numbers of units (64, 128) and dropout rates (0.2, 0.4, 0.6, 0.8) for each cell type. 
The experimental results are summarized in Table \ref{tab:AJRNNcelltype}. 
A learning rate of 0.001 and a batch size of 256 were used in all experiments, and the generator and discriminator were trained for the same number of epochs. 
Our findings show that the GRU cell outperforms the LSTM cell in all cases.

\begin{table}[H]
  \centering
  \begin{tabular}{cccr} 
      Cell type & Units & Dropout & Overall Accuracy\\[0.2cm] 
      \hline \\[-0.2cm] 
      GRU   &   64  & 0.2 &  $82.10 \pm 1.72$\\
      GRU   &   64  & 0.4 &  $81.75 \pm 1.18$\\
      GRU   &   64  & 0.6 &  $77.70 \pm 2.92$\\
      GRU   &   64  & 0.8 &  $78.78 \pm 2.26$\\[0.05cm] \hline \\[-0.25cm]

      GRU   &   128 & 0.2 &  $80.58 \pm 1.17$\\
      GRU   &   128 & 0.4 &  $85.84 \pm 1.68$\\
      GRU   &   128 & 0.6 &  $\mathbf{86.07 \pm 1.58}$\\
      GRU   &   128 & 0.8 &  $85.03 \pm 2.66$\\[0.05cm] \hline \\[-0.25cm]

      GRU   &   256 & 0.2 &  $80.49 \pm 1.47$\\
      GRU   &   256 & 0.4 &  $75.25 \pm 1.08$\\
      GRU   &   256 & 0.6 &  $75.97 \pm 1.23$\\
      GRU   &   256 & 0.8 &  $77.34 \pm 1.74$\\[0.05cm] \hline \\[-0.25cm]

      LSTM  &   64  & 0.2 &  $79.41 \pm 1.76$\\
      LSTM  &   64  & 0.4 &  $84.92 \pm 3.69$\\
      LSTM  &   64  & 0.6 &  $81.16 \pm 2.99$\\
      LSTM  &   64  & 0.8 &  $79.56 \pm 2.33$\\[0.05cm] \hline \\[-0.25cm]

      LSTM  &   128 & 0.2 &  $84.89 \pm 1.56$\\
      LSTM  &   128 & 0.4 &  $85.08 \pm 0.35$\\
      LSTM  &   128 & 0.6 &  $83.24 \pm 1.29$\\
      LSTM  &   128 & 0.8 &  $82.39 \pm 1.99$\\[0.05cm] \hline \\[-0.25cm]

      LSTM  &   256 & 0.2 &  $74.18 \pm 1.44$\\
      LSTM  &   256 & 0.4 &  $67.15 \pm 1.32$\\
      LSTM  &   256 & 0.6 &  $71.36 \pm 1.56$\\
      LSTM  &   256 & 0.8 &  $\mathbf{85.37 \pm 2.37}$\\ 
      
  \end{tabular}
  \caption{Overall Accuracy for GRU and LSTM.}
  \label{tab:AJRNNcelltype}
\end{table}

After conducting experiments with both LSTM and GRU cells with varying numbers of units and dropout rates, we determined that the GRU cell outperformed the LSTM cell.
As a result, we chose to further experiment with the GRU cell. 
We found that using a GRU cell with 128 units was a good compromise between model complexity and accuracy. 
This decision was made after careful consideration of the trade-off between the number of parameters in the model and its ability to accurately classify incomplete time series data.

We also investigated the effect of batch size on the overall accuracy of the model.
We tried different batch sizes and found that the batch size of 256 yielded the best results.
The results are summarized in Table \ref{tab:AJRNNbatchsize}.

\begin{table}[H]
  \centering
  \begin{tabular}{cccr} 
      Batch size & Learning rate & Dropout & Overall Accuracy\\[0.2cm] 
      \hline \\[-0.2cm]
      256 &   1e-03 &   0.0 & $80.13 \pm 2.20$\\
      256 &   1e-03 &   0.2 & $81.58 \pm 1.17$\\
      256 &   1e-03 &   0.4 & $85.85 \pm 1.39$\\
      256 &   1e-03 &   0.6 & $\mathbf{86.03 \pm 1.51}$\\
      256 &   1e-03 &   0.8 & $84.03 \pm 2.66$\\[0.05cm] \hline \\[-0.25cm]

      256 &   1e-04 &   0.4 & $80.00 \pm 1.91$\\
      256 &   1e-04 &   0.6 & $85.46 \pm 1.01$\\
      256 &   1e-04 &   0.8 & $83.88 \pm 1.33$\\[0.05cm] \hline \\[-0.25cm]

      32  &   1e-03 & 0.0 & $35.57 \pm 0.00$\\
      32  &   1e-03 & 0.5 & $29.32 \pm 0.00$\\[0.05cm] \hline \\[-0.25cm]

      32  &   1e-04 & 0.0 & $35.58 \pm 4.65$\\
      32  &   1e-04 & 0.5 & $36.27 \pm 8.39$\\
      32  &   1e-04 & 0.8 & $35.58 \pm 2.33$\\[0.05cm] \hline \\[-0.25cm]

      32  &   1e-05 & 0.0 & $56.12 \pm 0.88$\\
      32  &   1e-05 & 0.5 & $69.33 \pm 0.34$\\
      32  &   1e-05 & 0.8 & $67.12 \pm 1.23$\\[0.05cm] \hline \\[-0.25cm]

      32  &   1e-06 & 0.0 & $64.06 \pm 2.74$\\
      32  &   1e-06 & 0.5 & $66.58 \pm 3.78$\\[0.05cm] \hline \\[-0.25cm]

      32  &   1e-07 & 0.0 & $72.97 \pm 1.56$\\
      32  &   1e-07 & 0.5 & $71.70 \pm 1.45$\\[0.05cm] \hline \\[-0.25cm]

      32  &   1e-08 & 0.0 & $73.85 \pm 1.31$\\
      32  &   1e-08 & 0.5 & $72.92 \pm 1.40$\\[0.05cm] \hline \\[-0.25cm]

      32  &   1e-09 & 0.0 & $\mathbf{77.24 \pm 1.12}$\\
      32  &   1e-09 & 0.5 & $76.37 \pm 1.36$\\
  \end{tabular}
  \caption{Influence of batch size, learning rate and dropout on Overall Accuracy for the GRU network}
  \label{tab:AJRNNbatchsize}
\end{table}

During our experiments, we observed that the choice of batch size has a significant impact on the accuracy of the model, when the learning rate is was constant.
Specifically, we found that using a smaller batch size can increase the likelihood of over fitting the training data, while using a larger batch size can lead to under fitting.

To improve the overall accuracy of the model with a lower batch size, we attempted to decrease the learning rate.
However, while we did observe some improvement, we were not able to achieve the same level of accuracy as when using a higher batch size.
These findings highlight the importance of carefully selecting hyperparameters, such as batch size and learning rate, to achieve optimal performance in machine learning models.

We also explored the impact of increasing the number of the generator $G$ epochs for each training step.

\begin{table}[H]
  \centering
  \begin{tabular}{cccr} 
      Batch size & G epochs & Overall Accuracy\\[0.2cm] 
      \hline \\[-0.2cm]
      256 &   1 &  $\mathbf{86.03 \pm 1.51}$\\
      256 &   2 &  $72.58 \pm 1.17$\\
      256 &   5 &  $68.05 \pm 1.39$\\[0.05cm] \hline \\[-0.25cm]
      32  &   1 &  $\mathbf{77.24 \pm 1.12}$\\
      32  &   2 &  $57.21 \pm 1.46$\\
      32  &   5 &  $27.07 \pm 1.05$\\
  \end{tabular}
  \caption{Influence of G epochs for the GRU network}
  \label{tab:AJRNNGepochs}
\end{table}

Our analysis revealed that augmenting the number of $G$ epochs can result in overfitting the training data, as presented in Table \ref{tab:AJRNNGepochs}.


\subsection{Light AJ-RNN}

Due to the computational expense of the AJ-RNN model, we decided to conduct experiments with a lighter version of the model, referred to as the Light AJ-RNN.
The goal of this approach was to create a baseline model that could be used as a reference point for comparison with the more complex and computationally demanding AJ-RNN model. 

In order to achieve this, we used the Keras library to implement the Light AJ-RNN model.
Our approach involved removing the imputation of missing values from the original AJ-RNN model and utilizing the pre-imputed data as input to the model.
We removed the Discriminator and thus eliminated adversarial joint training.
The Light AJ-RNN model only retained the GRU network and the Classifier from the original AJ-RNN model.
This approach was aimed at achieving a reduction in computational costs while still maintaining a level of performance comparable to the original AJ-RNN model.

We conducted experiments with the Light AJ-RNN model using the same hyperparameters as the original AJ-RNN model.


\begin{table}[H]
  \centering
  \begin{tabular}{cccr} 
      Batch size & Learning rate & Dropout & Overall Accuracy\\[0.2cm] 
      \hline \\[-0.2cm]
      256 & 0.0 & 1e-03 & $75.14 \pm 1.85$\\
      256 & 0.5 & 1e-03 & $78.48 \pm 1.33$\\[0.05cm] \hline \\[-0.25cm]

      256 & 0.0 & 1e-04 & $87.25 \pm 1.20$\\
      256 & 0.5 & 1e-04 & $82.60 \pm 2.17$\\
      256 & 0.8 & 1e-04 & $\mathbf{87.29 \pm 1.08}$\\[0.05cm] \hline \\[-0.25cm]

      256 & 0.0 & 1e-05 & $83.29 \pm 2.85$\\
      256 & 0.0 & 1e-06 & $85.59 \pm 1.30$\\[0.05cm] \hline \\[-0.25cm]

      256 & 0.0 & 1e-07 & $81.72 \pm 1.32$\\
      256 & 0.5 & 1e-07 & $81.76 \pm 2.21$\\[0.05cm] \hline \\[-0.25cm]

      32  & 0.5 & 1e-03 & $30.85 \pm 1.35$\\[0.05cm] \hline \\[-0.25cm]

      32  & 0.0 & 1e-04 & $33.55 \pm 1.42$\\
      32  & 0.5 & 1e-04 & $32.33 \pm 1.33$\\[0.05cm] \hline \\[-0.25cm]

      32  & 0.0 & 1e-05 & $44.23 \pm 2.02$\\[0.05cm] \hline \\[-0.25cm]
      32  & 0.0 & 1e-06 & $54.32 \pm 3.73$\\[0.05cm] \hline \\[-0.25cm]
      32  & 0.0 & 1e-07 & $52.98 \pm 2.22$\\[0.05cm] \hline \\[-0.25cm]
      32  & 0.0 & 1e-08 & $\mathbf{55.80 \pm 1.27}$\\
      32  & 0.5 & 1e-08 & $52.32 \pm 1.36$\\
  \end{tabular}
  \caption{Overall accuracy of Light AJ-RNN model for different hyperparameters}
  \label{tab:LightAJRNNBatchSizeResults}
\end{table}

The results of the experiments are shown in Table \ref{tab:LightAJRNNBatchSizeResults}.

Despite our initial hopes that the Light AJ-RNN model would prove to be a viable alternative to the computationally expensive original AJ-RNN model, the results of our experiments showed that the overall accuracy of the Light AJ-RNN model was comparable to that of the original AJ-RNN model for most of the hyperparameter combinations tested.

Interestingly, despite using pre-imputed data as input to the model, we did not observe the expected improvement in overall accuracy that we had hoped for. 
While disappointing, this result was nonetheless informative and suggested that the original AJ-RNN model was indeed necessary for achieving the highest levels of accuracy in our application.

Therefore, we made the decision to halt further experimentation with the Light AJ-RNN model and instead focused our efforts on continuing to refine and improve the original AJ-RNN model. 


\subsection{Findings}

In the study conducted on the AJ-RNN model, a deep learning model for classifying time series while imputing missing data.
Our experiments aimed to explore the performance of the model and to find the best hyperparameters for our real-world dataset.

Overall, our findings were mixed. 
While the model showed promise on some of our experiments, we found that the overall accuracy on our real-world dataset was not as high as we expected.
It is worth noting that training RNNs is very computationally expensive, and we had to run many experiments to find the best hyperparameters.

We also found that the adversarial joint training is really sensitive to the hyperparameters.
We experimented with different learning rates, batch sizes, and dropout rates and found that these parameters can significantly impact the performance of the model.
Therefore, it is important to carefully tune these hyperparameters to achieve the best possible results.

In conclusion, we found that the AJ-RNN model can be an effective tool for imputing missing data. 
However, it is important to carefully consider the hyperparameters and to perform extensive experimentation to achieve the best possible results.

The model has been implemented in Python using the Keras library \cite{chollet2015keras} and the TensorFlow backend \cite{tensorflow2015-whitepaper}.
The source code for this model is available at \url{https://github.com/dnldsht/AJ-RNN}, which is a reimplemented and extended version of the initial AJ-RNN implementation \cite{ajrnn}.