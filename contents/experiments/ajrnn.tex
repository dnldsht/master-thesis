\section{AJ-RNN}
``Adversarial Joint-Learning Recurrent Neural Network for Incomplete Time Series Classification" \cite{ajrnn} is a research paper that proposes a novel approach for classification of incomplete time series data.
The authors begin by highlighting the challenges of working with incomplete time series data, particularly the difficulty of extracting features and the need to deal with missing values.

To address these challenges, the authors propose an adversarial joint-learning recurrent neural network (AJ-RNN) that uses a recurrent neural network (RNN) to capture the temporal dependencies in the time series data, and an adversarial learning approach to impute the missing values.

The AJ-RNN is trained using a joint optimization framework that alternates between training the RNN for classification and the imputation network to fill in the missing values.
The adversarial component of the imputation network is used to ensure that the imputed values are as close as possible to the true values.

The authors evaluate the performance of the AJ-RNN on several real-world datasets and demonstrate that it outperforms several existing state-of-the-art approaches for time series classification.

\subsection{Adversarial learning}

% TODO review
Previous studies have shown that adversarial approaches outperform traditional methods in generating data that conforms to the distribution of a given dataset \cite{goodfellow2014generative, ledig2017photo}.

Furthermore, GANs have shown promising results in filling in missing data in time series prediction \cite{yoon2018gain, li2018learning, luo2018multivariate}. Similarly, adversarial techniques have also been applied to tasks such as video captioning \cite{yang2018video} and domain adaptation \cite{ganin2017domain}.
However, prior to this paper, the question of how to apply adversarial learning in the domain of incomplete time series classification (ITSC) has not been explored.

The integration of adversarial training and joint learning in recurrent neural networks (RNNs) is explored in this paper, resulting in the development of a system called Adversarial Joint learning RNN (AJ-RNN).

The study's results show that employing AJ-RNN, an end-to-end framework integrating adversarial training and joint learning in recurrent neural networks, is an effective solution to tackle the issue of incomplete time series classification.
J-RNN is trained to predict the value of the next input variable when it is revealed, and to fill in the missing value with its prediction.
At the same time, AJ-RNN also learns to classify.
Hence AJ-RNN can directly perform classification with missing values.

\subsection{Model}

% TODO rephrase

In this section, we present our model, the Adversarial Joint learning Recurrent Neural Network (AJ-RNN), to address the issue of time series classification with missing values.
The framework is shown in Figure \ref{fig:AJRNNrchitecture}. Here we describe how adversarial and joint learning strategies are integrated into an RNN.

We denote a time series $X = {x_1, x_2, ..., x_T }$ as a sequence vector of $T$ observations, where each observation $x_t \in R^d$.

Suppose a time series $X$ has missing values which can be indicated by a $T$-dimensional mask vector $M = {m_1, m_2, ..., m_T}$, where $m_t \in {0, 1} \in R^d$, $m_t$ is $1$ if $x_t$ is revealed, and $0$ if $x_t$ is missing. 
Since we focus on incomplete time series classification, there is a target label $y^{(i)}$ for $i$-th time series $X^i$ given the time series data set $D$, where $D = \{(X^i,M^i, y^i)\}^N_{i=1}$.

To avoid the traditional two-step approach, we regard imputation and classification as two tasks in multitask learning.
Specifically, the AJ-RNN is trained to approximate the value of the next input variable, using it as a target when it is revealed, while if the next value is missing, it will be filled in with the current prediction. 
At the same time, the AJ-RNN is trained to perform the classification task.

We solve the missing value problem by two processes: approximation and imputation. 
As seen in Fig. \ref{fig:AJRNNrchitecture}, two kinds of links enable AJ-RNN to directly model time series in the presence of missing values: dashed blue links (for approximation) and solid blue links (for imputation). 
We train $\hat{x_t}$ to approximate the next value $x_t$ when it is revealed using the last hidden states $h_{t-1}$ as follows:

\begin{equation}
  \hat{x_t} = W_{imp} h_{t-1} + b_z
\end{equation}

where $W_{imp} \in R^{n \times  m}$ is a learned regression matrix and $b_z$ is a bias term. 
Since $\hat{x_t}$ is trained to approximate the next value $x_t$, it can be used to impute it when missing. 
The input value $u_t$ is computed as:

\begin{equation}
  u_t = m_t \odot x_t + (1 - m_t) \odot \hat{x_t}
  \label{eq:AJRNNinput}
\end{equation}

where $m_t$ is the mask as defined above and $\odot$ is the element-wise product.

The RNN is trained using the completed input value $u_t$.
Formally, the update equation of the RNN is:

\begin{equation}
  h_t = F_{RNN} (h_{t-1}, u_t; W)
\end{equation}

Where $h_t$ represents the hidden unit vector at time $t$, $W$ encapsulates the input-to-hidden and hidden-to-hidden parameters, and $F_{RNN}$ represents the update function of the particular RNN variant.

Finally, the last hidden state of the RNN, $h_T$ , is fed into the classifier to obtain the probability distribution over each category label using a softmax:
\begin{equation}
  P(\hat{y_j}|h_T ) = \frac{exp(W^T_j  h_T )}{\sum_{l=1}^K exp(W^T_j  h_T )}
  \label{eq:AJRNNsoftmax}
\end{equation}

where $K$ is the number of class labels and $\{W_l\}^K_{l=1}$ are the class-specific weights of the softmax layer.
The classifier can use more complicated networks depending on the task.
We use this simple classifier because our main goal is to demonstrate mitigation of the exploding bias problem and report on the results achieved.
% For fairness, we use this approach for all methods, ours and the methods we implemented for comparison.

There are two tasks during the joint learning process, namely, imputation and classification. 
Let the superscript $i$ denote the $i$-th sample of the time series data set $D$.
For the imputation task, we can obtain the imputation sequence vector $\hat{X^i} = \{ \hat{x}^i_2, ..., \hat{x}^i_T \}$ 
of the $i$-th time series sample which can be divided into two parts, approximation values
(orange units in Fig. \ref{fig:AJRNNrchitecture}) and imputation ones (purple units
in Fig. \ref{fig:AJRNNrchitecture}). 
The imputation loss of all time series samples is calculated on the approximation values as follows:

\begin{equation}
  \mathcal{L}_{imp}(X, \hat{X}, M) = \frac{1}{N} \sum_{i=1}^N \lVert (X^i_{2:T} - \hat{X}^i) \odot M^i_{2:T} \rVert_2^2
  \label{eq:AJRNNimploss}
\end{equation}

where $N$ is the number of samples in the data set.
Equation (\ref{eq:AJRNNimploss}) is the mean squared error loss between the approximation and the revealed values.
$ M^i_{2:T}$ masks off the imputation values from the imputation loss since there is no ground truth for the missing values.

For the classification task, we obtain the predicted probability distribution $\hat{y}^i$ of $i$-th time series sample given by
Equation (\ref{eq:AJRNNsoftmax}), the loss of all samples of a time series can be calculated as follows:

\begin{equation}
  \mathcal{L}_{cls}(y, \hat{y}) = - \frac{1}{N} \sum_{i=1}^N \sum_{j=1}^K 1\{ y^i = j\} \log\hat{y}^i 
  \label{eq:AJRNNclsloss}
\end{equation}

where $K$ is the number of class labels. Equation (\ref{eq:AJRNNclsloss}) is the softmax cross entropy loss of the predicted label and the true label.

This end-to-end framework inevitably suffers from the negative impact (error propagation from imputation to classification) of missing values because the imputed values will contain some amount of error simply due to the fact that they are predictions.
This error can be quickly amplified as it is fed into the RNN, resulting in the exploding bias problem.

Here, we introduce a discriminator $D$ to reduce the negative impact of missing values. 
The discriminator $D$ distinguishes whether each value in the completed vector is real or imputed, rather than identifying the entire completed vector. 
In this way, the discriminator $D$ can provide direct supervision to the imputed values.

Specifically, for the $i$-th time series training sample, there is a completed sequence vector $U^i = \{u^i_2 , ..., u^i_T\}$
given by Equation (\ref{eq:AJRNNinput}) and its corresponding mask vector $M^i_{2:T} = \{m^i_2, ..., m^i_T \}$.

In $U^i$, some are real values, while the rest are imputed.
We know which are which by the mask vector $M^i$ . We can take advantage of this knowledge to provide a supervision signal for the imputation network.

The adversarial learning strategy is defined as a two player minimax game.
We alternately update the parameters of discriminator $D$ and the parameters of the AJ-RNN.
First, $D$ receives the completed sequence vector as input and is trained to distinguish which values in the completed sequence vector are revealed and which are imputed. 
The discriminator loss can be defined as follows:

\begin{align}
  \mathcal{L}_{D}(U, M) & = - [E \log(D(X_{real})) + E \log(1 - D(\hat{X}_{imp}))] \label{eq:AJRNNdloss} \\ 
                        & = - \frac{1}{N} \sum_{i=1}^N \left[ M^i_{2:T} \odot \log(D(U^i)) + (1 - M^i_{2:T}) \odot \log(1-D(U^i)) \right]
\end{align}

where $D(\cdot)$ denotes the estimated mask probability $\hat{P}$ of the discriminator. 
In Equation (\ref{eq:AJRNNdloss}), the first term is the log output of the discriminator on real values, and the discriminator tries to maximize this to 1.
The second term is the loss for imputed values.
Hence the discriminator tries to minimize its output for imputed values.

The discriminator is composed of 3 fully connected layers to take advantage of global contextual information (since it takes a sequence as input), where the number of units in the hidden layers are set to $T$ (i.e., the length of the sample), $T/2$, and $T$, respectively.
We use tanh as the activation function of each layer except for the output layer where we use the sigmoid activation function to get the estimated real/fake probability.

The RNN is thus supervised to make the distribution of predicted values is closer to the revealed ones by fooling the discriminator $D$. 
The adversarial loss of the RNN can be defined as follows:

\begin{equation}
  \mathcal{L}_{adv}(U, M) = \frac{1}{N} \sum_{i=1}^N  (1 - M^i_{2:T}) \odot \log(1 - D(U^i))
  \label{eq:AJRNNadvloss}
\end{equation}

Hence the AJ-RNN tries to maximize the discriminator output for imputed values.
This provides a supervisory signal for each imputed value in the completed sequence vector, which can reduce the bias introduced by the imputation operation and thus alleviate the exploding bias problem.
Note that we can also feed the whole imputation vector $\hat{X}^i$ into the discriminator, providing supervision for both the approximated and imputed values. 
In our experiments, we found that the results obtained by these two methods are similar. 
Hence, considering computational efficiency, we adopted Equation (\ref{eq:AJRNNadvloss}) as the adversarial loss.

Finally, the overall training loss of AJ-RNN is defined as follows:

\begin{equation}
  \mathcal{L}_{AJ-RNN} = \mathcal{L}_{cls} + \mathcal{L}_{imp} + \lambda_d \mathcal{L}_{adv}
  \label{eq:AJRNNloss}
\end{equation}

where $\lambda_d$ is hyper-parameter. This forms an end-to-end
training framework for incomplete time series classification.

AJ-RNN combines the merits of joint learning and adversarial learning.
The discriminator $D$ is trained with the revealed values and the mask vector effectively provides supervision on each imputed value. 
Therefore, the negative impact of missing values on AJ-RNN is reduced.

\begin{figure}[ht]
  \centering
  \includegraphics[width=1\textwidth]{ajrnn}
  \caption{The proposed AJ-RNN framework. We use green units to denote revealed inputs, yellow for the output of approximated values, purple for
  the imputed values and a red “X” for missing inputs. Dashed links are for approximation training and solid ones are for imputation. The discriminator
  receives a completed vector composed of revealed and imputed values as input, which provides a one-to-one supervisory signal for imputed values
  $\hat{x_3}$ and $\hat{x_4}$. \cite{ajrnn}}
  \label{fig:AJRNNrchitecture}
\end{figure}

\subsection{Data preparation}
As described in Section \ref{sec:tempCNNDataPreparation}, we divided our dataset into three separate subsets, namely training, validation, and test, in a 60:20:20 ratio, respectively. 
Our goal was to ensure that each subset had a similar class distribution and that there was no spatial autocorrelation between them.

We understood the significance of the mask vector in the AJ-RNN model, which is used to differentiate between revealed and imputed values.
Therefore, we generated a mask vector for each sample to feed into the model.

\subsection{Experimental results}

The AJ-RNN model proposed in the paper was implemented in Python 2.7 and Tensorflow 1.0.
However, given the changes in technology and advances in the field since the publication of the paper, we found it necessary to re-implement the model in a more recent version of TensorFlow.
As such, we spent a considerable amount of time and effort to re-implement the model in Tensorflow 2.0 in a modularized fashion, with the aim of enhancing code readability and facilitating experimentation with different configurations.

In addition to the re-implementation of the model, we also performed a careful validation process to ensure that the accuracy achieved by our model was consistent with the results reported in the original paper, using the same datasets. 
This involved testing the model on a range of datasets and comparing the results with the original paper. 
By doing so, we were able to ensure that our implementation was both accurate and reliable.

Furthermore, we extended the implementation of the model to support multivariate time series data. 
This undertaking was quite significant as it required extensive additional coding and validation efforts. 
However, it was essential to ensure that the AJ-RNN model could be effectively utilized in various applications, including our own.


We tried different combinations of hyper-parameters to find the best configuration for our dataset.
The hyper-parameters we tuned are the RNN cell type, the units of the cell, the learning rate, the batch size, and the number of epochs for the generator and discriminator.

In our experiments, we investigated the performance of the AJ-RNN model with both LSTM and GRU cells. 
We tested different numbers of units (64, 128) and dropout rates (0.2, 0.4, 0.6, 0.8) for each cell type. 
The experimental results are summarized in Table \ref{tab:AJRNNcelltype}. 
A learning rate of 0.001 and a batch size of 256 were used in all experiments, and the generator and discriminator were trained for the same number of epochs. 
Our findings show that the GRU cell outperforms the LSTM cell in all cases.

\begin{table}[!htbp]
  \centering
  \begin{tabular}{cccr} 
      Cell type & Units & Dropout & Overall Accuracy\\[0.2cm] 
      \hline \\[-0.2cm] 
      GRU   &   64  & 0.2 &  $84.10 \pm 3.72$\\
      GRU   &   64  & 0.4 &  $81.75 \pm 4.18$\\
      GRU   &   64  & 0.6 &  $79.70 \pm 4.92$\\
      GRU   &   64  & 0.8 &  $82.78 \pm 4.26$\\[0.05cm] \hline \\[-0.25cm]
      GRU   &   128 & 0.2 &  $78.58 \pm 4.17$\\
      GRU   &   128 & 0.4 &  $80.84 \pm 0.00$\\
      GRU   &   128 & 0.6 &  $70.07 \pm 9.58$\\
      GRU   &   128 & 0.8 &  $81.03 \pm 4.66$\\[0.05cm] \hline \\[-0.25cm]
      GRU   &   256 & 0.2 &  $80.49 \pm 8.77$\\
      GRU   &   256 & 0.4 &  $75.25 \pm 1.88$\\
      GRU   &   256 & 0.6 &  $75.97 \pm 1.43$\\
      GRU   &   256 & 0.8 &  $77.34 \pm 1.74$\\[0.05cm] \hline \\[-0.25cm]
      LSTM  &   64  & 0.2 &  $79.41 \pm 1.76$\\
      LSTM  &   64  & 0.4 &  $84.92 \pm 3.69$\\
      LSTM  &   64  & 0.6 &  $81.16 \pm 4.99$\\
      LSTM  &   64  & 0.8 &  $0.00 \pm 0.00$\\[0.05cm] \hline \\[-0.25cm]
      LSTM  &   128 & 0.2 &  $84.89 \pm 3.00$\\
      LSTM  &   128 & 0.4 &  $85.08 \pm 0.35$\\
      LSTM  &   128 & 0.6 &  $83.24 \pm 4.29$\\
      LSTM  &   128 & 0.8 &  $82.39 \pm 1.99$\\[0.05cm] \hline \\[-0.25cm]
      LSTM  &   256 & 0.2 &  $74.18 \pm 3.44$\\
      LSTM  &   256 & 0.4 &  $67.15 \pm 15.02$\\
      LSTM  &   256 & 0.6 &  $71.36 \pm 22.56$\\
      LSTM  &   256 & 0.8 &  $85.37 \pm 2.37$\\ 
      
  \end{tabular}
  \caption{Overall Accuracy for GRU and LSTM.}
  \label{tab:AJRNNcelltype}
\end{table}

After conducting experiments with both LSTM and GRU cells with varying numbers of units and dropout rates, we determined that the GRU cell outperformed the LSTM cell in all cases.
As a result, we chose to further experiment with the GRU cell. 
We found that using a GRU cell with 128 units was a good compromise between model complexity and accuracy. 
This decision was made after careful consideration of the trade-off between the number of parameters in the model and its ability to accurately classify incomplete time series data.

We also investigated the effect of batch size on the overall accuracy of the model.
We tried different batch sizes (32, 64, 128, 256, 512) and found that the batch size of 256 yielded the best results.
The results are summarized in Table \ref{tab:AJRNNbatchsize}.

During our experiments, we observed that the choice of batch size has a significant impact on the accuracy of the model, when the learning rate is was constant.
Specifically, we found that using a smaller batch size can increase the likelihood of over fitting the training data, while using a larger batch size can lead to under fitting.

To improve the overall accuracy of the model with a lower batch size, we attempted to decrease the learning rate.
However, while we did observe some improvement, we were not able to achieve the same level of accuracy as when using a higher batch size.
These findings highlight the importance of carefully selecting hyperparameters, such as batch size and learning rate, to achieve optimal performance in machine learning models.


\begin{table}[!htbp]
  \centering
  \begin{tabular}{cccr} 
      Batch size & Learning rate & Dropout & Overall Accuracy\\[0.2cm] 
      \hline \\[-0.2cm]
      256 &   1e-03 &   0.0 & $86.13 \pm 0.00$\\
      256 &   1e-03 &   0.2 & $78.58 \pm 4.17$\\
      256 &   1e-03 &   0.4 & $81.05 \pm 4.39$\\
      256 &   1e-03 &   0.6 & $73.03 \pm 9.81$\\
      256 &   1e-03 &   0.8 & $81.03 \pm 4.66$\\[0.05cm] \hline \\[-0.25cm]

      256 &   1e-04 &   0.4 & $80.00 \pm 2.91$\\
      256 &   1e-04 &   0.6 & $85.46 \pm 2.01$\\
      256 &   1e-04 &   0.8 & $83.88 \pm 4.33$\\[0.05cm] \hline \\[-0.25cm]

      32  &   1e-03 &   0.2 & $32.45 \pm 4.42$\\
      32  &   1e-03 &   0.6 & $28.21 \pm 6.00$\\
      32  &   1e-03 &   0.8 & $23.07 \pm 1.05$\\
      32  &   1e-04 &   0.0 & $30.26 \pm 0.00$\\[0.05cm] \hline \\[-0.25cm]

      32  &   1e-04 &   0.2 & $39.49 \pm 5.91$\\
      32  &   1e-04 &   0.4 & $28.93 \pm 0.00$\\
      32  &   1e-04 &   0.6 & $25.64 \pm 0.00$\\
      32  &   1e-04 &   0.8 & $32.97 \pm 0.00$\\[0.05cm] \hline \\[-0.25cm]

      32  &   1e-05 &   0.8 & $67.12 \pm 0.00$\\[0.05cm] \hline \\[-0.25cm]
      32  &   1e-08 &   0.0 & $73.32 \pm 2.83$
  \end{tabular}
  \caption{Influence of batch size, learning rate and dropout on Overall Accuracy for the GRU network}
  \label{tab:AJRNNbatchsize}
\end{table}


\pagebreak
\subsection{Light AJ-RNN}

Due to the computational expense of the AJ-RNN model, we decided to conduct experiments with a lighter version of the model, referred to as the Light AJ-RNN.
The goal of this approach was to create a baseline model that could be used as a reference point for comparison with the more complex and computationally demanding AJ-RNN model. 

In order to achieve this, we used the Keras library to implement the Light AJ-RNN model.
Our approach involved removing the imputation of missing values from the original AJ-RNN model and utilizing the pre-imputed data as input to the model.
We removed the Discriminator and thus eliminated adversarial joint training.
The Light AJ-RNN model only retained the GRU network and Classifier from the original AJ-RNN model.
This approach was aimed at achieving a reduction in computational costs while still maintaining a level of performance comparable to the original AJ-RNN model.

We conducted experiments with the Light AJ-RNN model using the same hyperparameters as the original AJ-RNN model.


\begin{table}[!htbp]
  \centering
  \begin{tabular}{cccr} 
      Batch size & Learning rate & Dropout & Overall Accuracy\\[0.2cm] 
      \hline \\[-0.2cm]
      256 & 0.0 & 1e-03 & $75.14 \pm 6.85$\\
      256 & 0.5 & 1e-03 & $78.48 \pm 0.00$\\[0.05cm] \hline \\[-0.25cm]

      256 & 0.0 & 1e-04 & $87.25 \pm 1.20$\\
      256 & 0.5 & 1e-04 & $82.60 \pm 2.17$\\
      256 & 0.8 & 1e-04 & $87.29 \pm 1.08$\\[0.05cm] \hline \\[-0.25cm]

      256 & 0.0 & 1e-05 & $83.29 \pm 2.85$\\
      256 & 0.0 & 1e-06 & $85.59 \pm 0.00$\\[0.05cm] \hline \\[-0.25cm]

      256 & 0.0 & 1e-07 & $81.72 \pm 3.32$\\
      256 & 0.5 & 1e-07 & $81.76 \pm 3.21$\\[0.05cm] \hline \\[-0.25cm]

      32  & 0.5 & 1e-03 & $30.85 \pm 0.00$\\[0.05cm] \hline \\[-0.25cm]

      32  & 0.0 & 1e-04 & $33.55 \pm 2.42$\\
      32  & 0.5 & 1e-04 & $32.33 \pm 1.33$\\[0.05cm] \hline \\[-0.25cm]

      32  & 0.0 & 1e-05 & $44.23 \pm 12.02$\\[0.05cm] \hline \\[-0.25cm]
      32  & 0.0 & 1e-06 & $54.32 \pm 13.73$\\[0.05cm] \hline \\[-0.25cm]
      32  & 0.0 & 1e-07 & $52.98 \pm 15.22$\\[0.05cm] \hline \\[-0.25cm]
      32  & 0.0 & 1e-08 & $55.80 \pm 11.27$\\
      32  & 0.5 & 1e-08 & $52.32 \pm 11.36$\\
  \end{tabular}
  \caption{Overall accuracy of Light AJ-RNN model for different hyperparameters}
  \label{tab:LightAJRNNBatchSizeResults}
\end{table}

The results of the experiments are shown in Table \ref{tab:LightAJRNNBatchSizeResults}.

Despite our initial hopes that the Light AJ-RNN model would prove to be a viable alternative to the computationally expensive original AJ-RNN model, the results of our experiments showed that the overall accuracy of the Light AJ-RNN model was comparable to that of the original AJ-RNN model for most of the hyperparameter combinations tested.

Interestingly, despite using pre-imputed data as input to the model, we did not observe the expected improvement in overall accuracy that we had hoped for. 
While disappointing, this result was nonetheless informative and suggested that the original AJ-RNN model was indeed necessary for achieving the highest levels of accuracy in our application.

Therefore, we made the decision to halt further experimentation with the Light AJ-RNN model and instead focused our efforts on continuing to refine and improve the original AJ-RNN model. 


\subsection{Findings}

In the study conducted on the AJ-RNN model, a deep learning model for classifying time series while imputing missing data.
Our experiments aimed to explore the performance of the model and to find the best hyperparameters for our real-world dataset.

Overall, our findings were mixed. 
While the model showed promise on some of our experiments, we found that the overall accuracy on our real-world dataset was not as high as we expected.
It is worth noting that training RNNs is very computationally expensive, and we had to run many experiments to find the best hyperparameters.

We also found that the adversarial joint training is really sensitive to the hyperparameters.
We experimented with different learning rates, batch sizes, and dropout rates and found that these parameters can significantly impact the performance of the model.
Therefore, it is important to carefully tune these hyperparameters to achieve the best possible results.

In conclusion, we found that the AJ-RNN model can be an effective tool for imputing missing data. 
However, it is important to carefully consider the hyperparameters and to perform extensive experimentation to achieve the best possible results.

